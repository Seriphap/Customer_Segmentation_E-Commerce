import streamlit as st
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
import plotly.graph_objects as go
import requests
import time
import json

def run(df: pd.DataFrame):
    st.subheader("Customer Segmentation using K-Means")

    # ========= 0) Validate & Preprocess =========
    required_cols = {"Quantity", "UnitPrice", "InvoiceNo", "CustomerID", "InvoiceDate", "Amount"}
    missing = required_cols - set(df.columns)
    if missing:
        st.error(f"Missing required columns: {sorted(missing)}")
        return

    # Ensure datetime
    if not np.issubdtype(df["InvoiceDate"].dtype, np.datetime64):
        df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"], errors="coerce")

    # Basic cleaning
    df = df[df["Quantity"] > 0]
    df = df[df["UnitPrice"] > 0]
    df = df[~df["InvoiceNo"].astype(str).str.startswith("C")]
    df = df.dropna(subset=["CustomerID", "InvoiceDate"])

    # RFM base (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Cluster)
    snapshot_date = df["InvoiceDate"].max() + pd.Timedelta(days=1)
    rfm = df.groupby("CustomerID").agg(
        Recency=("InvoiceDate", lambda x: (snapshot_date - x.max()).days),
        Frequency=("InvoiceNo", "nunique"),
        Monetary=("Amount", "sum"),
    )

    # ========= 1) UI: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å K ‡πÅ‡∏•‡∏∞‡∏õ‡∏∏‡πà‡∏°‡∏£‡∏±‡∏ô (‡∏Å‡∏±‡∏ô rerun) =========
    with st.form("rfm_form"):
        k = st.slider("Select number of clusters (K)", 2, 10, 4)
        submitted = st.form_submit_button("Run clustering & explain")

    # ========= 2) Helper: backoff + prompt + cache =========
    def call_gemini_with_backoff(model, prompt: str,
                                 max_retries: int = 5,
                                 base: int = 2,
                                 max_sleep: int = 30):
        """Retry ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ 429 RESOURCE_EXHAUSTED ‡∏î‡πâ‡∏ß‡∏¢ truncated exponential backoff"""
        for i in range(max_retries):
            try:
                return model.generate_content(prompt)
            except ResourceExhausted:
                sleep = min(max_sleep, base ** i)
                time.sleep(sleep)
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡πÇ‡∏¢‡∏ô‡∏ï‡πà‡∏≠
        raise

    def build_explain_prompt(summary_rows):
        """
        summary_rows: list[dict] ‡πÄ‡∏ä‡πà‡∏ô
        [{"Cluster":0,"Recency":43.70,"Frequency":3.68,"Monetary":1359.05,"CustomerCount":3054}, ...]
        """
        return f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å RFM (Recency, Frequency, Monetary)
‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á (‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå) ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö

‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
- ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡πà‡∏≠‡∏¢', '‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏´‡∏•‡∏∏‡∏î' ‡∏Ø‡∏•‡∏Ø)
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå 1-2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏Ñ‡∏°‡πÄ‡∏õ‡∏ç 2 ‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á + ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠) ‡πÄ‡∏õ‡πá‡∏ô bullet point
- ‡∏†‡∏≤‡∏©‡∏≤: ‡πÑ‡∏ó‡∏¢
- ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ RFM: Recency ‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢ = ‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î; Frequency ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å = ‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡πà‡∏≠‡∏¢; Monetary ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å = ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏™‡∏π‡∏á

Cluster summary (JSON):
{json.dumps(summary_rows, ensure_ascii=False)}
""".strip()

    @st.cache_data(show_spinner=False, ttl=3600)
    def explain_cached(model_name: str, summary_key: tuple, summary_rows: list):
        """‡πÅ‡∏Ñ‡∏ä‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° summary (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏¥‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°)"""
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        model = genai.GenerativeModel(model_name)
        prompt = build_explain_prompt(summary_rows)
        resp = call_gemini_with_backoff(model, prompt)
        return resp.text

    # ========= 3) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î Submit: ‡∏ó‡∏≥ KMeans + Summary + ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI =========
    if submitted:
        # Scale & Fit
        scaler = StandardScaler()
        rfm_scaled = scaler.fit_transform(rfm)

        # ‡∏ï‡∏±‡πâ‡∏á n_init ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö scikit-learn ‡πÉ‡∏´‡∏°‡πà ‡πÜ
        km = KMeans(n_clusters=k, random_state=42, n_init="auto")
        labels = km.fit_predict(rfm_scaled)

        rfm_with_cluster = rfm.copy()
        rfm_with_cluster["Cluster"] = labels

        # Summary 4√ó4
        summary = (
            rfm_with_cluster
            .groupby("Cluster")
            .agg(Recency=("Recency", "mean"),
                 Frequency=("Frequency", "mean"),
                 Monetary=("Monetary", "mean"),
                 CustomerCount=("Cluster", "count"))
            .round(2)
            .reset_index()
        )

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ (‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏≤‡∏Å session_state ‡πÅ‡∏°‡πâ rerun)
        st.write("**Cluster Summary**")
        st.dataframe(summary, use_container_width=True)

        # Silhouette (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)
        if len(set(labels)) > 1 and len(labels) > len(set(labels)):
            try:
                sil = silhouette_score(rfm_scaled, labels)
                st.caption(f"Silhouette score: {sil:.3f}")
            except Exception:
                pass

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM: list[dict] (‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å)
        summary_rows = summary.to_dict(orient="records")

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÅ‡∏Ñ‡∏ä‡∏ó‡∏µ‡πà hashable (‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°)
        summary_key = tuple(tuple(sorted(d.items())) for d in summary_rows)

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Gemini 2.0 Flash..."):
            explanation = explain_cached("gemini-2.0-flash", summary_key, summary_rows)
            st.session_state["explanation"] = explanation

    # ========= 4) ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• AI ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏°‡πâ rerun =========
    if "explanation" in st.session_state:
        st.subheader("ü§ñ Gemini Analysis of Clusters")
        st.write(st.session_state["explanation"])

    #------------------------------------------------------------
    
    st.session_state['rfm'] = rfm
    st.session_state['rfm_scaled'] = rfm_scaled
    st.session_state['model'] = model

    st.subheader("RFM Variable Descriptions")
    text_explain = {
            "‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ (Variable)": ["Recency", "Frequency", "Monetary"],
            "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai Description)": [
                "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏ô‡πâ‡∏≠‡∏¢ = ‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡πá‡∏ß ‡πÜ ‡∏ô‡∏µ‡πâ)",
                "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ã‡∏∑‡πâ‡∏≠ (‡∏°‡∏≤‡∏Å = ‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡πà‡∏≠‡∏¢)",
                "‡∏¢‡∏≠‡∏î‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏£‡∏ß‡∏° (‡∏°‡∏≤‡∏Å = ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á)"
            ],
            "English Description": [
                "Number of days since the last purchase (lower = more recent)",
                "Number of purchases (higher = more frequent)",
                "Total spending amount (higher = more valuable customer)"
            ]
        }
    st.dataframe(pd.DataFrame(text_explain),hide_index=True)

    # ... download buttons for each cluster ...
    st.subheader("Download All Cluster Data")

    # Reset index to include CustomerID in the CSV
    all_clusters_csv = rfm.reset_index().to_csv(index=False).encode('utf-8')

    st.download_button(
        label="Download All Clusters (CSV)",
        data=all_clusters_csv,
        file_name='all_clusters_data.csv',
        mime='text/csv'
    )

    # Elbow method & Silhouette score
    sse = []
    silhouette_scores = []
    k_range = range(2, 11)
    for k in k_range:
        model = KMeans(n_clusters=k, random_state=42)
        labels = model.fit_predict(rfm_scaled)
        sse.append(model.inertia_)
        silhouette_scores.append(silhouette_score(rfm_scaled, labels))

    # Plot Elbow Method using Plotly
    st.write("### Elbow Method")
    elbow_fig = go.Figure()
    elbow_fig.add_trace(go.Scatter(x=list(k_range), y=sse, mode='lines+markers', name='SSE'))
    elbow_fig.update_layout(title='Elbow Method For Optimal K',
                            xaxis_title='Number of clusters (K)',
                            yaxis_title='Sum of Squared Errors (SSE)',
                            width=1400,
                            height=600)
    st.plotly_chart(elbow_fig)

    # Plot Silhouette Score using Plotly
    st.write("### Silhouette Score")
    silhouette_fig = go.Figure()
    silhouette_fig.add_trace(go.Scatter(x=list(k_range), y=silhouette_scores, mode='lines+markers',
                                        name='Silhouette Score', line=dict(color='green')))
    silhouette_fig.update_layout(title='Silhouette Score For Optimal K',
                                 xaxis_title='Number of clusters (K)',
                                 yaxis_title='Silhouette Score',
                                width=1400,
                                height=600)
    st.plotly_chart(silhouette_fig)
