import streamlit as st
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
import plotly.graph_objects as go
import requests
import time
import json

def run(df: pd.DataFrame):
    st.subheader("Customer Segmentation using K-Means")

    # ========= 0) Validate & Preprocess =========
    required_cols = {"Quantity", "UnitPrice", "InvoiceNo", "CustomerID", "InvoiceDate", "Amount"}
    missing = required_cols - set(df.columns)
    if missing:
        st.error(f"Missing required columns: {sorted(missing)}")
        return

    # Ensure datetime
    if not np.issubdtype(df["InvoiceDate"].dtype, np.datetime64):
        df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"], errors="coerce")

    # Basic cleaning
    df = df[df["Quantity"] > 0]
    df = df[df["UnitPrice"] > 0]
    df = df[~df["InvoiceNo"].astype(str).str.startswith("C")]
    df = df.dropna(subset=["CustomerID", "InvoiceDate"])

    # RFM base (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Cluster)
    snapshot_date = df["InvoiceDate"].max() + pd.Timedelta(days=1)
    rfm = df.groupby("CustomerID").agg(
        Recency=("InvoiceDate", lambda x: (snapshot_date - x.max()).days),
        Frequency=("InvoiceNo", "nunique"),
        Monetary=("Amount", "sum"),
    )

    # ========= 1) UI: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å K ‡πÅ‡∏•‡∏∞‡∏õ‡∏∏‡πà‡∏°‡∏£‡∏±‡∏ô (‡∏Å‡∏±‡∏ô rerun) =========
    with st.form("rfm_form"):
        k = st.slider("Select number of clusters (K)", 2, 10, 4)
        submitted = st.form_submit_button("Run clustering & explain")

    # ========= 2) Helper: backoff + prompt + cache =========
    def call_gemini_with_backoff(model, prompt: str,
                                 max_retries: int = 5,
                                 base: int = 2,
                                 max_sleep: int = 30):
        """Retry ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ 429 RESOURCE_EXHAUSTED ‡∏î‡πâ‡∏ß‡∏¢ truncated exponential backoff"""
        for i in range(max_retries):
            try:
                return model.generate_content(prompt)
            except ResourceExhausted:
                sleep = min(max_sleep, base ** i)
                time.sleep(sleep)
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡πÇ‡∏¢‡∏ô‡∏ï‡πà‡∏≠
        raise

    def build_explain_prompt(summary_rows):
        """
        summary_rows: list[dict] ‡πÄ‡∏ä‡πà‡∏ô
        [{"Cluster":0,"Recency":43.70,"Frequency":3.68,"Monetary":1359.05,"CustomerCount":3054}, ...]
        """
        return f"""
        ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å RFM (Recency, Frequency, Monetary)
        ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á (‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå) ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
        
        ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î:
        - ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡πà‡∏≠‡∏¢', '‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏´‡∏•‡∏∏‡∏î' ‡∏Ø‡∏•‡∏Ø)
        - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå 1-2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏Ñ‡∏°‡πÄ‡∏õ‡∏ç 2 ‡∏Ç‡πâ‡∏≠‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á + ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠) ‡πÄ‡∏õ‡πá‡∏ô bullet point
        - ‡∏†‡∏≤‡∏©‡∏≤: ‡πÑ‡∏ó‡∏¢
        - ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ RFM: Recency ‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢ = ‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î; Frequency ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å = ‡∏°‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡πà‡∏≠‡∏¢; Monetary ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å = ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏™‡∏π‡∏á
        
        Cluster summary (JSON):
        {json.dumps(summary_rows, ensure_ascii=False)}
        """.strip()

    @st.cache_data(show_spinner=False, ttl=3600)
    def explain_cached(model_name: str, summary_key: tuple, summary_rows: list):
        """‡πÅ‡∏Ñ‡∏ä‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° summary (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏¥‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°)"""
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        model = genai.GenerativeModel(model_name)
        prompt = build_explain_prompt(summary_rows)
        resp = call_gemini_with_backoff(model, prompt)
        return resp.text

    # ========= 3) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î Submit: ‡∏ó‡∏≥ KMeans + Summary + ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI =========
    if submitted:
        # Scale & Fit
        scaler = StandardScaler()
        rfm_scaled = scaler.fit_transform(rfm)

        # ‡∏ï‡∏±‡πâ‡∏á n_init ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö scikit-learn ‡πÉ‡∏´‡∏°‡πà ‡πÜ
        km = KMeans(n_clusters=k, random_state=42, n_init="auto")
        labels = km.fit_predict(rfm_scaled)

        rfm_with_cluster = rfm.copy()
        rfm_with_cluster["Cluster"] = labels

        # Summary 4√ó4
        summary = (
            rfm_with_cluster
            .groupby("Cluster")
            .agg(Recency=("Recency", "mean"),
                 Frequency=("Frequency", "mean"),
                 Monetary=("Monetary", "mean"),
                 CustomerCount=("Cluster", "count"))
            .round(2)
            .reset_index()
        )

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ (‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏≤‡∏Å session_state ‡πÅ‡∏°‡πâ rerun)
        st.write("**Cluster Summary**")
        st.dataframe(summary, use_container_width=True)

        # Silhouette (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)
        if len(set(labels)) > 1 and len(labels) > len(set(labels)):
            try:
                sil = silhouette_score(rfm_scaled, labels)
                st.caption(f"Silhouette score: {sil:.3f}")
            except Exception:
                pass

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM: list[dict] (‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å)
        summary_rows = summary.to_dict(orient="records")

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÅ‡∏Ñ‡∏ä‡∏ó‡∏µ‡πà hashable (‡πÑ‡∏°‡πà‡∏¢‡∏¥‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°)
        summary_key = tuple(tuple(sorted(d.items())) for d in summary_rows)

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Gemini 2.0 Flash..."):
            explanation = explain_cached("gemini-2.0-flash", summary_key, summary_rows)
            st.session_state["explanation"] = explanation

    # ========= 4) ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• AI ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏°‡πâ rerun =========
    if "explanation" in st.session_state:
        st.subheader("ü§ñ Gemini Analysis of Clusters")
        st.write(st.session_state["explanation"])

                                height=600)
    st.plotly_chart(silhouette_fig)
